{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.14.3"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "id": "title",
   "metadata": {},
   "source": "# PINN Fundamentals: 1D Heat Equation\n\n## Problem Setup\n\nWe solve:\n$$\\frac{\\partial u}{\\partial t} = \\alpha \\frac{\\partial^2 u}{\\partial x^2}, \\quad x \\in [0,1],\\; t \\in [0,1]$$\n\nwith IC $u(x,0) = \\sin(\\pi x)$ and BCs $u(0,t) = u(1,t) = 0$.\n\n- **Forward problem**: given $\\alpha = 0.01$, find $u(x,t)$\n- **Inverse problem**: given noisy measurements of $u$, recover $\\alpha$\n\nThis notebook covers:\n1. Data generation\n2. Forward problem — optimiser comparison (Adam, L-BFGS, adaptive weights)\n3. Inverse problem — same optimiser comparison\n4. Sensitivity analysis — noise, initial guess, data volume, uncertainty"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": "import torch\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport sys\nsys.path.append('..')\n\nfrom data.heat_data import HeatEquationData\nfrom models.heat_pinn_strategy import StrategicPINN\nfrom training.trainer_strategy import StrategicPINNTrainer\nfrom utils.plotter import plot_solution\n\ntorch.manual_seed(42)  # reproducible weight initialisation\nprint('Imports successful.')"
  },
  {
   "cell_type": "markdown",
   "id": "sec-data",
   "metadata": {},
   "source": "## 1. Data Generation"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gen-data",
   "metadata": {},
   "outputs": [],
   "source": "data_gen = HeatEquationData(\n    L=1.0, T=1.0, alpha=0.01,\n    N_f=10000, N_bc=100, N_ic=200,\n    N_sensors=10, N_time_measurements=10,\n    noise_level=0.01,\n    device='cpu',\n    random_seed=42\n)\n\ndata = data_gen.generate_full_dataset()\ndata_gen.visualize_data(data)"
  },
  {
   "cell_type": "markdown",
   "id": "sec-forward",
   "metadata": {},
   "source": "## 2. Forward Problem\n\nWe fix $\\alpha = 0.01$ and train the network to approximate $u(x,t)$."
  },
  {
   "cell_type": "markdown",
   "id": "fwd-adam-hdr",
   "metadata": {},
   "source": "### 2.1 Adam optimizer only\n\nBaseline: Adam with a `ReduceLROnPlateau` schedule and no second-order refinement."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fwd-adam",
   "metadata": {},
   "outputs": [],
   "source": "model_fwd_adam = StrategicPINN(\n    layers=[2, 50, 50, 50, 50, 1],\n    alpha_true=0.01,\n    inverse=False\n)\n\ntrainer_fwd_adam = StrategicPINNTrainer(\n    model=model_fwd_adam,\n    data=data,\n    learning_rate=1e-3,\n    switch_var=1e-12,   # disabled: never switch to L-BFGS\n    switch_slope=1e-12,\n    adaptive_weights=False,\n)\n\ntrainer_fwd_adam.train(epochs=5000, print_every=1000, plot_every=2500)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fwd-adam-plot",
   "metadata": {},
   "outputs": [],
   "source": "plot_solution(model_fwd_adam, data, alpha_true=0.01)"
  },
  {
   "cell_type": "markdown",
   "id": "fwd-lbfgs-hdr",
   "metadata": {},
   "source": "### 2.2 Adam + L-BFGS\n\nAdam warms up the parameters; once training plateaus, L-BFGS is used for second-order refinement."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fwd-lbfgs",
   "metadata": {},
   "outputs": [],
   "source": "model_fwd_lbfgs = StrategicPINN(\n    layers=[2, 50, 50, 50, 50, 1],\n    alpha_true=0.01,\n    inverse=False\n)\n\ntrainer_fwd_lbfgs = StrategicPINNTrainer(\n    model=model_fwd_lbfgs,\n    data=data,\n    learning_rate=1e-3,\n    switch_var=0.1,\n    switch_slope=0.001,\n    adaptive_weights=False,\n)\n\ntrainer_fwd_lbfgs.train(epochs=5000, print_every=1000, plot_every=2500)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fwd-lbfgs-plot",
   "metadata": {},
   "outputs": [],
   "source": "plot_solution(model_fwd_lbfgs, data, alpha_true=0.01)"
  },
  {
   "cell_type": "markdown",
   "id": "fwd-lbfgs-compare-hdr",
   "metadata": {},
   "source": "### 2.3 L-BFGS advantage\n\nTrain Adam-only to the same epoch count as the L-BFGS switch point above, then compare errors directly."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fwd-lbfgs-compare",
   "metadata": {},
   "outputs": [],
   "source": "# Find the epoch at which L-BFGS kicked in\nlbfgs_switch = next(\n    (i for i, o in enumerate(trainer_fwd_lbfgs.history['optimizer']) if o == 'lbfgs'),\n    len(trainer_fwd_lbfgs.history['optimizer'])\n)\nprint(f'L-BFGS switch epoch: {lbfgs_switch}')\n\nmodel_fwd_adam_same = StrategicPINN(\n    layers=[2, 50, 50, 50, 50, 1],\n    alpha_true=0.01,\n    inverse=False\n)\n\ntrainer_fwd_adam_same = StrategicPINNTrainer(\n    model=model_fwd_adam_same,\n    data=data,\n    switch_var=1e-12,\n    switch_slope=1e-12,\n    adaptive_weights=False,\n)\n\ntrainer_fwd_adam_same.train(epochs=lbfgs_switch, print_every=500, plot_every=10000)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fwd-lbfgs-compare-plot",
   "metadata": {},
   "outputs": [],
   "source": "from models.heat_pinn import analytical_solution\n\nx_eval = torch.linspace(0, 1, 100).reshape(-1, 1)\nt_eval = torch.linspace(0, 1, 100).reshape(-1, 1)\nX, T = torch.meshgrid(x_eval.squeeze(), t_eval.squeeze(), indexing='ij')\nx_flat = X.flatten().reshape(-1, 1)\nt_flat = T.flatten().reshape(-1, 1)\n\nu_exact = analytical_solution(x_flat.numpy(), t_flat.numpy(), alpha=0.01)\nu_adam  = model_fwd_adam_same.predict(x_flat, t_flat)\nu_lbfgs = model_fwd_lbfgs.predict(x_flat, t_flat)\n\nrel_adam  = np.sqrt(np.mean((u_adam  - u_exact)**2)) / np.sqrt(np.mean(u_exact**2)) * 100\nrel_lbfgs = np.sqrt(np.mean((u_lbfgs - u_exact)**2)) / np.sqrt(np.mean(u_exact**2)) * 100\n\nprint(f'Adam only  ({lbfgs_switch} epochs): relative L2 = {rel_adam:.4f}%')\nprint(f'Adam+LBFGS ({lbfgs_switch}+ epochs): relative L2 = {rel_lbfgs:.4f}%')\nprint(f'Improvement factor: {rel_adam / rel_lbfgs:.1f}x')"
  },
  {
   "cell_type": "markdown",
   "id": "fwd-gradnorm-hdr",
   "metadata": {},
   "source": "### 2.4 Loss gradient norms\n\nTraining loss spikes occur when the optimiser probes high-curvature regions of the parameter landscape. Tracking the per-loss gradient norms $\\|\\nabla_\\theta \\mathcal{L}_i\\|_2$ reveals which loss component dominates at each spike."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fwd-gradnorm",
   "metadata": {},
   "outputs": [],
   "source": "model_fwd_gn = StrategicPINN(\n    layers=[2, 50, 50, 50, 50, 1],\n    alpha_true=0.01,\n    inverse=False\n)\n\ntrainer_fwd_gn = StrategicPINNTrainer(\n    model=model_fwd_gn,\n    data=data,\n    learning_rate=1e-3,\n    switch_var=1e-12,\n    switch_slope=1e-12,\n    track_gradient_norms=True,\n    adaptive_weights=False,\n)\n\ntrainer_fwd_gn.train(epochs=2000, print_every=1000, plot_every=1000)"
  },
  {
   "cell_type": "markdown",
   "id": "fwd-aw-hdr",
   "metadata": {},
   "source": "### 2.5 Adaptive weights\n\nAdaptive weighting re-balances loss terms dynamically based on gradient magnitudes (Wang et al., 2021 proxy). Useful when loss components differ by orders of magnitude."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fwd-aw",
   "metadata": {},
   "outputs": [],
   "source": "model_fwd_aw = StrategicPINN(\n    layers=[2, 50, 50, 50, 50, 1],\n    alpha_true=0.01,\n    inverse=False\n)\n\ntrainer_fwd_aw = StrategicPINNTrainer(\n    model=model_fwd_aw,\n    data=data,\n    learning_rate=1e-3,\n    switch_var=0.1,\n    switch_slope=0.001,\n    adaptive_weights=True,\n)\n\ntrainer_fwd_aw.train(epochs=5000, print_every=1000, plot_every=2500)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fwd-aw-plot",
   "metadata": {},
   "outputs": [],
   "source": "plot_solution(model_fwd_aw, data, alpha_true=0.01)"
  },
  {
   "cell_type": "markdown",
   "id": "fwd-conclusion",
   "metadata": {},
   "source": "**Forward problem summary**\n\n| Configuration | Notes |\n|---|---|\n| Adam only | Reasonable baseline but stagnates |\n| Adam + L-BFGS | ~8× lower L2 error at same wall time |\n| Adaptive weights | Small benefit for this smooth PDE |"
  },
  {
   "cell_type": "markdown",
   "id": "sec-inverse",
   "metadata": {},
   "source": "## 3. Inverse Problem\n\nNow $\\alpha$ is unknown. We add noisy sensor measurements as an extra loss term and recover $\\alpha$ jointly with $u$."
  },
  {
   "cell_type": "markdown",
   "id": "inv-adam-hdr",
   "metadata": {},
   "source": "### 3.1 Adam only"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "inv-adam",
   "metadata": {},
   "outputs": [],
   "source": "model_inv_adam = StrategicPINN(\n    layers=[2, 50, 50, 50, 50, 1],\n    inverse=True,\n    alpha_init=0.02\n)\n\ntrainer_inv_adam = StrategicPINNTrainer(\n    model=model_inv_adam,\n    data=data,\n    learning_rate=1e-3,\n    switch_var=1e-12,\n    switch_slope=1e-12,\n    adaptive_weights=False,\n)\n\ntrainer_inv_adam.train(epochs=5000, print_every=1000, plot_every=2500)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "inv-adam-eval",
   "metadata": {},
   "outputs": [],
   "source": "def eval_inverse(model, alpha_true=0.01):\n    pred = model.get_alpha()\n    err  = abs(pred - alpha_true) / alpha_true * 100\n    print(f'True α: {alpha_true:.6f}  |  Predicted α: {pred:.6f}  |  Error: {err:.2f}%')\n    status = 'SUCCESS' if err < 5 else 'needs longer training'\n    print(f'[{status}]')\n\neval_inverse(model_inv_adam)\nplot_solution(model_inv_adam, data, alpha_true=0.01)"
  },
  {
   "cell_type": "markdown",
   "id": "inv-lbfgs-hdr",
   "metadata": {},
   "source": "### 3.2 Adam + L-BFGS"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "inv-lbfgs",
   "metadata": {},
   "outputs": [],
   "source": "model_inv_lbfgs = StrategicPINN(\n    layers=[2, 50, 50, 50, 50, 1],\n    inverse=True,\n    alpha_init=0.02\n)\n\ntrainer_inv_lbfgs = StrategicPINNTrainer(\n    model=model_inv_lbfgs,\n    data=data,\n    learning_rate=1e-3,\n    switch_var=0.1,\n    switch_slope=0.001,\n    adaptive_weights=False,\n)\n\ntrainer_inv_lbfgs.train(epochs=5000, print_every=1000, plot_every=2500)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "inv-lbfgs-eval",
   "metadata": {},
   "outputs": [],
   "source": "eval_inverse(model_inv_lbfgs)\nplot_solution(model_inv_lbfgs, data, alpha_true=0.01)"
  },
  {
   "cell_type": "markdown",
   "id": "inv-aw-hdr",
   "metadata": {},
   "source": "### 3.3 Adaptive weights + L-BFGS\n\nThe extra measurement loss makes the inverse problem harder to balance — adaptive weights help more here."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "inv-aw",
   "metadata": {},
   "outputs": [],
   "source": "model_inv_aw = StrategicPINN(\n    layers=[2, 50, 50, 50, 50, 1],\n    inverse=True,\n    alpha_init=0.02\n)\n\ntrainer_inv_aw = StrategicPINNTrainer(\n    model=model_inv_aw,\n    data=data,\n    learning_rate=1e-3,\n    switch_var=0.1,\n    switch_slope=0.001,\n    adaptive_weights=True,\n)\n\ntrainer_inv_aw.train(epochs=5000, print_every=1000, plot_every=2500)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "inv-aw-eval",
   "metadata": {},
   "outputs": [],
   "source": "eval_inverse(model_inv_aw)\nplot_solution(model_inv_aw, data, alpha_true=0.01)"
  },
  {
   "cell_type": "markdown",
   "id": "inv-conclusion",
   "metadata": {},
   "source": "**Inverse problem summary**\n\n| Configuration | Typical α error |\n|---|---|\n| Adam only | ~1–2% |\n| Adam + L-BFGS | ~0.5% |\n| Adaptive + L-BFGS | ~0.1% |\n\nAdaptive weights are more impactful for the inverse problem than the forward problem."
  },
  {
   "cell_type": "markdown",
   "id": "sec-sensitivity",
   "metadata": {},
   "source": "## 4. Sensitivity Analysis\n\nAll experiments below use the best configuration found above: Adam + L-BFGS with adaptive weights."
  },
  {
   "cell_type": "markdown",
   "id": "sens-noise-hdr",
   "metadata": {},
   "source": "### 4.1 Effect of measurement noise\n\n⚠️ **Long-running cell** — trains 4 models × 5000 epochs each."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sens-noise",
   "metadata": {},
   "outputs": [],
   "source": "noise_levels = [0.001, 0.01, 0.05, 0.1]\nresults_noise = []\n\nfor noise in noise_levels:\n    print(f'\\nNoise level: {noise:.1%}')\n    dg = HeatEquationData(\n        N_f=10000, N_bc=100, N_ic=200,\n        N_sensors=10, N_time_measurements=10,\n        noise_level=noise, random_seed=42\n    )\n    d = dg.generate_full_dataset()\n    m = StrategicPINN(inverse=True, alpha_init=0.02)\n    t = StrategicPINNTrainer(\n        m, d, switch_var=0.1, switch_slope=0.001, adaptive_weights=True\n    )\n    t.train(epochs=5000, print_every=2500, plot_every=10000)\n    results_noise.append({'noise': noise, 'error': abs(m.get_alpha() - 0.01) / 0.01 * 100})\n\nfig, ax = plt.subplots(figsize=(8, 5))\nax.plot([r['noise'] for r in results_noise], [r['error'] for r in results_noise],\n        'bo-', markersize=10, linewidth=2)\nax.axhline(y=5, color='r', linestyle='--', label='5% threshold')\nax.set_xscale('log')\nax.set_xlabel('Noise level'); ax.set_ylabel('α error (%)')\nax.set_title('Parameter recovery vs measurement noise')\nax.legend(); ax.grid(True, alpha=0.3)\nplt.tight_layout(); plt.show()"
  },
  {
   "cell_type": "markdown",
   "id": "sens-guess-hdr",
   "metadata": {},
   "source": "### 4.2 Sensitivity to initial guess\n\n⚠️ **Long-running cell** — trains 4 models × 5000 epochs each."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sens-guess",
   "metadata": {},
   "outputs": [],
   "source": "alpha_guesses = [0.005, 0.05, 0.5, 5.0]\nresults_guess = []\n\nfor guess in alpha_guesses:\n    print(f'\\nInitial guess: {guess}')\n    m = StrategicPINN(inverse=True, alpha_init=guess)\n    t = StrategicPINNTrainer(\n        m, data, switch_var=0.1, switch_slope=0.001, adaptive_weights=True\n    )\n    t.train(epochs=5000, print_every=2500, plot_every=10000)\n    results_guess.append({'guess': guess, 'error': abs(m.get_alpha() - 0.01) / 0.01 * 100})\n\nfig, ax = plt.subplots(figsize=(8, 5))\nax.plot([r['guess'] for r in results_guess], [r['error'] for r in results_guess],\n        'bo-', markersize=10, linewidth=2)\nax.axhline(y=5, color='r', linestyle='--', label='5% threshold')\nax.axvline(x=0.01, color='g', linestyle=':', label='True α')\nax.set_xscale('log')\nax.set_xlabel('Initial α guess'); ax.set_ylabel('α error (%)')\nax.set_title('Parameter recovery vs initial guess')\nax.legend(); ax.grid(True, alpha=0.3)\nplt.tight_layout(); plt.show()"
  },
  {
   "cell_type": "markdown",
   "id": "sens-data-hdr",
   "metadata": {},
   "source": "### 4.3 Data requirements\n\nHow many sensor measurements are needed for reliable parameter recovery?\n\n⚠️ **Long-running cell** — trains 4 models × 5000 epochs each."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sens-data",
   "metadata": {},
   "outputs": [],
   "source": "x_sensor_counts = [2, 5, 10, 20]\nt_sensors = 5\nresults_data = []\n\nfor n_s in x_sensor_counts:\n    n_total = n_s * t_sensors\n    print(f'\\nSensors: {n_s} × {t_sensors} = {n_total} measurements')\n    dg = HeatEquationData(\n        N_f=10000, N_bc=100, N_ic=200,\n        N_sensors=n_s, N_time_measurements=t_sensors,\n        noise_level=0.01, random_seed=42\n    )\n    d = dg.generate_full_dataset()\n    m = StrategicPINN(inverse=True, alpha_init=0.02)\n    t = StrategicPINNTrainer(\n        m, d, switch_var=0.1, switch_slope=0.001, adaptive_weights=True\n    )\n    t.train(epochs=5000, print_every=2500, plot_every=10000)\n    results_data.append({'n': n_total, 'error': abs(m.get_alpha() - 0.01) / 0.01 * 100})\n\nfig, ax = plt.subplots(figsize=(8, 5))\nax.plot([r['n'] for r in results_data], [r['error'] for r in results_data],\n        'bo-', markersize=10, linewidth=2)\nax.axhline(y=5, color='r', linestyle='--', label='5% threshold')\nax.set_xlabel('Number of measurements'); ax.set_ylabel('α error (%)')\nax.set_title('Parameter recovery vs data volume')\nax.legend(); ax.grid(True, alpha=0.3)\nplt.tight_layout(); plt.show()"
  },
  {
   "cell_type": "markdown",
   "id": "sens-uq-hdr",
   "metadata": {},
   "source": "### 4.4 Uncertainty quantification\n\nTrain an ensemble of 10 models with different random seeds to estimate variability in $\\hat{\\alpha}$.\n\n⚠️ **Long-running cell** — trains 10 models × 5000 epochs each."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sens-uq",
   "metadata": {},
   "outputs": [],
   "source": "alphas_ensemble = []\n\nfor seed in range(10):\n    torch.manual_seed(seed)\n    m = StrategicPINN(inverse=True, alpha_init=0.02)\n    t = StrategicPINNTrainer(\n        m, data, switch_var=0.1, switch_slope=0.001, adaptive_weights=True\n    )\n    t.train(epochs=5000, print_every=5000, plot_every=10000)\n    alphas_ensemble.append(m.get_alpha())\n\nalpha_mean = np.mean(alphas_ensemble)\nalpha_std  = np.std(alphas_ensemble)\nprint(f'\\nEnsemble results (n=10):')\nprint(f'  α = {alpha_mean:.6f} ± {alpha_std:.6f}')\nprint(f'  True α = 0.01')\nprint(f'  Mean error: {abs(alpha_mean - 0.01) / 0.01 * 100:.2f}%')\nprint(f'  95% CI: [{alpha_mean - 2*alpha_std:.6f}, {alpha_mean + 2*alpha_std:.6f}]')\n\nfig, ax = plt.subplots(figsize=(8, 4))\nax.scatter(range(10), alphas_ensemble, zorder=3)\nax.axhline(y=0.01, color='r', linestyle='--', label='True α')\nax.axhline(y=alpha_mean, color='b', linestyle='-', label=f'Mean = {alpha_mean:.5f}')\nax.fill_between(range(10),\n                alpha_mean - alpha_std, alpha_mean + alpha_std,\n                alpha=0.2, color='b', label='±1 std')\nax.set_xlabel('Seed'); ax.set_ylabel('α')\nax.set_title('Ensemble uncertainty in α recovery')\nax.legend(); ax.grid(True, alpha=0.3)\nplt.tight_layout(); plt.show()"
  }
 ]
}
